{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9IJm0WVLIthN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-0.6B-Base\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen3-0.6B-Base\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ycKSBI4kJGsS",
        "outputId": "1a0005aa-02dd-4881-d752-236dff05e7e8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9hi_w08fJYu4",
        "outputId": "6e026047-82a9-4ad8-c3c7-3cc1068cb06f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "train_dataset = load_dataset('csv', data_files = '/content/drive/MyDrive/contradictory-my-dear-watson/data/train.csv')\n",
        "test_dataset = load_dataset('csv', data_files = '/content/drive/MyDrive/contradictory-my-dear-watson/data/test.csv')"
      ],
      "metadata": {
        "id": "w8Q_XlquJdjp"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9XZ5BEZ-JxSY",
        "outputId": "5fac4dc9-9204-4d2e-96a2-d621feb828f5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'premise', 'hypothesis', 'lang_abv', 'language', 'label'],\n",
              "        num_rows: 12120\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_dataset['train'][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ei3LFWkvJyN9",
        "outputId": "63c68d4a-c63a-4dd7-907f-fb8d40142552"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'id': '5130fd2cb5', 'premise': 'and these comments were considered in formulating the interim rules.', 'hypothesis': 'The rules developed in the interim were put together with these comments in mind.', 'lang_abv': 'en', 'language': 'English', 'label': 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ULz-FPoJ02s",
        "outputId": "607f3ab2-14fd-434d-a88f-5d3168869707"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'premise', 'hypothesis', 'lang_abv', 'language'],\n",
              "        num_rows: 5195\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_dataset['train'][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZFjo2sUDKLiR",
        "outputId": "51d589c2-36bd-4992-908a-6160e10b0d51"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'id': 'c6d58c3f69', 'premise': 'بکس، کیسی، راہیل، یسعیاہ، کیلی، کیلی، اور کولمبین ہائی اسکول کے دوسرے طلبا کے نام سے بکسوں کو نشان زد کیا جائے گا جس نے اس سال پہلے اپنی زندگی کھو دی', 'hypothesis': 'کیسی کے لئے کوئی یادگار نہیں ہوگا, کولمین ہائی اسکول کے طالب علموں میں سے ایک جو مر گیا.', 'lang_abv': 'ur', 'language': 'Urdu'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# split the training dataset into train/validation\n",
        "split_dataset = train_dataset['train'].train_test_split(\n",
        "    test_size=0.1,\n",
        "    seed=42,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "train_ds_orig = split_dataset['train']\n",
        "val_ds_orig = split_dataset['test']\n",
        "test_ds_orig = test_dataset['train']"
      ],
      "metadata": {
        "id": "sGP6Y-kiJ2Tw"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Training dataset length: {len(train_ds_orig)}\")\n",
        "print(f\"Validation dataset length: {len(val_ds_orig)}\")\n",
        "print(f\"Test dataset length: {len(test_ds_orig)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xeP9aQvzKFDb",
        "outputId": "5b4fb55d-ed2c-49c3-a791-41e64380122a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training dataset length: 10908\n",
            "Validation dataset length: 1212\n",
            "Test dataset length: 5195\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "icl_rows = train_ds_orig[10:20]\n",
        "print(icl_rows)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "veZekwbfQeiA",
        "outputId": "8d164017-9b13-4d6e-9fde-526d11b37ab2"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'id': ['dad3652214', '2c7a23e67d', '3d18b90d52', 'aee859bf43', '5459a3f905', '6ced0f8989', '07acbc212e', '2cc2f555de', '3cb6c5e9a5', 'b2550cab04'], 'premise': ['There would be little benefit to national saving from allowing early access to mandatory accounts with set contribution levels-which has been proposed for Social Security (see Q4.', 'เรายังไม่ได้สัมภาษณ์บุคคลที่มีความรู้ความสามารถครบทุกคนหรือยังไม่ได้เห็นรายงานที่เกี่ยวข้องทั้งหมดเลย', 'The most important directions are simply up and up leads eventually to the cathedral and fortress commanding the hilltop, and down inevitably leads to one of three gates through the wall to the new town.', 'Jon walked back to the town to the smithy.', 'في الوقت الحالي تم فتح ممر في التصنيف للرجال ومن هذا الممر أتت السيدة بيشوب تليها المرأة التي أسلافها من الزنوج.', 'Ωωωω, είναι υπέροχη, είναι ξέρετε, είναι ένας χαρακτήρας που θα καθίσει με οποιονδήποτε, θα παίξει με οποιονδήποτε', \"yeah it's just a matter of education i think\", 'तो इससे कोई फर्क नहीं पड़ता,  केवल हंगामा होता है', \"wow who can afford that  my God i can't afford to miss a day let alone six\", 'To control land and sea routes to the south, the Mauryas still needed to conquer the eastern kingdom of Kalinga (modern Orissa).'], 'hypothesis': ['There would be little benefit to national saving', 'ยังไม่ได้รับข้อมูลจากทุกคนที่รู้', 'Go downwards to one of the gates, all of which will lead you into the cathedral.', 'Jon continued on into the mountains.', 'مشىت الانسة بيشوب خلال مجموعة من الرجال.', 'Αυτή συνήθως παίζει πόκερ ή μπλακ τζακ αλλά μερικές φορές παίζει και σκραμπλ.', \"Yeah but education doesn't matter.\", 'कुछ लोग अराजकता फैलाना पसंद करते हैं |', 'If I needed to take time off from work, I could afford it.', 'The Mauryas had a large army capable of conquering Kalinga.'], 'lang_abv': ['en', 'th', 'en', 'en', 'ar', 'el', 'en', 'hi', 'en', 'en'], 'language': ['English', 'Thai', 'English', 'English', 'Arabic', 'Greek', 'English', 'Hindi', 'English', 'English'], 'label': [0, 0, 2, 2, 0, 1, 2, 1, 2, 1]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.encode(\"yes\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2G-gKFdUHwN",
        "outputId": "c25ff510-3267-4d2d-9b61-29c2788ddcac"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[9693]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.encode(\"no\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MitLY4X3UKUx",
        "outputId": "deb64b23-064b-4aae-dbe9-28360b56d46d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2152]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.encode(\"maybe\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_fgnojgUMDv",
        "outputId": "82b0c4f7-e6db-4183-b11c-934b643d57b1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[36760]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CA0RvdaoUNvN",
        "outputId": "72c15345-888e-4a7f-86dd-7e4e8b73693e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Qwen2TokenizerFast(name_or_path='Qwen/Qwen3-0.6B-Base', vocab_size=151643, model_max_length=131072, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|endoftext|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
              "\t151643: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t151644: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t151645: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t151646: AddedToken(\"<|object_ref_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t151647: AddedToken(\"<|object_ref_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t151648: AddedToken(\"<|box_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t151649: AddedToken(\"<|box_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t151650: AddedToken(\"<|quad_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t151651: AddedToken(\"<|quad_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t151652: AddedToken(\"<|vision_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t151653: AddedToken(\"<|vision_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t151654: AddedToken(\"<|vision_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t151655: AddedToken(\"<|image_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t151656: AddedToken(\"<|video_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t151657: AddedToken(\"<tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
              "\t151658: AddedToken(\"</tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
              "\t151659: AddedToken(\"<|fim_prefix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
              "\t151660: AddedToken(\"<|fim_middle|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
              "\t151661: AddedToken(\"<|fim_suffix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
              "\t151662: AddedToken(\"<|fim_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
              "\t151663: AddedToken(\"<|repo_name|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
              "\t151664: AddedToken(\"<|file_sep|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
              "\t151665: AddedToken(\"<tool_response>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
              "\t151666: AddedToken(\"</tool_response>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
              "\t151667: AddedToken(\"<think>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
              "\t151668: AddedToken(\"</think>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
              "}\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index = 9\n",
        "\n",
        "premise = icl_rows['premise'][index]\n",
        "hypothesis = icl_rows['hypothesis'][index]\n",
        "label = icl_rows['label'][index]\n",
        "\n",
        "# 0 for entailment, 1 for neutral, 2 for contradiction\n",
        "label_to_text_map = {\n",
        "    0: \"yes\",\n",
        "    1: \"maybe\",\n",
        "    2: \"no\"\n",
        "}\n",
        "\n",
        "label_text = label_to_text_map[label]\n"
      ],
      "metadata": {
        "id": "KSanx37IVW_C"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create an icl prompt using the premis and hypthoseis\n",
        "template = \"Consider the following premise: {premise}.\\nDoes the premise entail the following hypothesis: {hypothesis}?\\nPlease answer with: 'yes', 'no', or 'maybe'.\\n{label_text}\""
      ],
      "metadata": {
        "id": "9NhMHYvNRbuX"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example = template.format(premise = premise, hypothesis = hypothesis, label_text = label_text)\n",
        "print(example)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-bWBFPHzUJX-",
        "outputId": "86265557-b3c2-42e6-f04b-c74f6a69c085"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Consider the following premise: To control land and sea routes to the south, the Mauryas still needed to conquer the eastern kingdom of Kalinga (modern Orissa)..\n",
            "Does the premise entail the following hypothesis: The Mauryas had a large army capable of conquering Kalinga.?\n",
            "Please answer with: 'yes', 'no', or 'maybe'.\n",
            "maybe\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create the icl prompt\n",
        "prompt = \"\"\n",
        "\n",
        "for index in range(10):\n",
        "  premise = icl_rows['premise'][index]\n",
        "  hypothesis = icl_rows['hypothesis'][index]\n",
        "  label = icl_rows['label'][index]\n",
        "\n",
        "  # 0 for entailment, 1 for neutral, 2 for contradiction\n",
        "  label_to_text_map = {\n",
        "      0: \"yes\",\n",
        "      1: \"maybe\",\n",
        "      2: \"no\"\n",
        "  }\n",
        "\n",
        "  label_text = label_to_text_map[label]\n",
        "\n",
        "  example = template.format(premise = premise, hypothesis = hypothesis, label_text = label_text)\n",
        "  prompt = prompt + example + \"\\n\\n\"\n",
        "\n",
        "print(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lh433nz1qQtw",
        "outputId": "6616b5df-2ead-49a6-ef87-935a1c94ee22"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Consider the following premise: There would be little benefit to national saving from allowing early access to mandatory accounts with set contribution levels-which has been proposed for Social Security (see Q4..\n",
            "Does the premise entail the following hypothesis: There would be little benefit to national saving?\n",
            "Please answer with: 'yes', 'no', or 'maybe'.\n",
            "yes\n",
            "\n",
            "Consider the following premise: เรายังไม่ได้สัมภาษณ์บุคคลที่มีความรู้ความสามารถครบทุกคนหรือยังไม่ได้เห็นรายงานที่เกี่ยวข้องทั้งหมดเลย.\n",
            "Does the premise entail the following hypothesis: ยังไม่ได้รับข้อมูลจากทุกคนที่รู้?\n",
            "Please answer with: 'yes', 'no', or 'maybe'.\n",
            "yes\n",
            "\n",
            "Consider the following premise: The most important directions are simply up and up leads eventually to the cathedral and fortress commanding the hilltop, and down inevitably leads to one of three gates through the wall to the new town..\n",
            "Does the premise entail the following hypothesis: Go downwards to one of the gates, all of which will lead you into the cathedral.?\n",
            "Please answer with: 'yes', 'no', or 'maybe'.\n",
            "no\n",
            "\n",
            "Consider the following premise: Jon walked back to the town to the smithy..\n",
            "Does the premise entail the following hypothesis: Jon continued on into the mountains.?\n",
            "Please answer with: 'yes', 'no', or 'maybe'.\n",
            "no\n",
            "\n",
            "Consider the following premise: في الوقت الحالي تم فتح ممر في التصنيف للرجال ومن هذا الممر أتت السيدة بيشوب تليها المرأة التي أسلافها من الزنوج..\n",
            "Does the premise entail the following hypothesis: مشىت الانسة بيشوب خلال مجموعة من الرجال.?\n",
            "Please answer with: 'yes', 'no', or 'maybe'.\n",
            "yes\n",
            "\n",
            "Consider the following premise: Ωωωω, είναι υπέροχη, είναι ξέρετε, είναι ένας χαρακτήρας που θα καθίσει με οποιονδήποτε, θα παίξει με οποιονδήποτε.\n",
            "Does the premise entail the following hypothesis: Αυτή συνήθως παίζει πόκερ ή μπλακ τζακ αλλά μερικές φορές παίζει και σκραμπλ.?\n",
            "Please answer with: 'yes', 'no', or 'maybe'.\n",
            "maybe\n",
            "\n",
            "Consider the following premise: yeah it's just a matter of education i think.\n",
            "Does the premise entail the following hypothesis: Yeah but education doesn't matter.?\n",
            "Please answer with: 'yes', 'no', or 'maybe'.\n",
            "no\n",
            "\n",
            "Consider the following premise: तो इससे कोई फर्क नहीं पड़ता,  केवल हंगामा होता है.\n",
            "Does the premise entail the following hypothesis: कुछ लोग अराजकता फैलाना पसंद करते हैं |?\n",
            "Please answer with: 'yes', 'no', or 'maybe'.\n",
            "maybe\n",
            "\n",
            "Consider the following premise: wow who can afford that  my God i can't afford to miss a day let alone six.\n",
            "Does the premise entail the following hypothesis: If I needed to take time off from work, I could afford it.?\n",
            "Please answer with: 'yes', 'no', or 'maybe'.\n",
            "no\n",
            "\n",
            "Consider the following premise: To control land and sea routes to the south, the Mauryas still needed to conquer the eastern kingdom of Kalinga (modern Orissa)..\n",
            "Does the premise entail the following hypothesis: The Mauryas had a large army capable of conquering Kalinga.?\n",
            "Please answer with: 'yes', 'no', or 'maybe'.\n",
            "maybe\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda')\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GDCERtsouDzG",
        "outputId": "a3a1c891-f403-4901-ba79-3fbc8e31256e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Qwen3ForCausalLM(\n",
              "  (model): Qwen3Model(\n",
              "    (embed_tokens): Embedding(151936, 1024)\n",
              "    (layers): ModuleList(\n",
              "      (0-27): 28 x Qwen3DecoderLayer(\n",
              "        (self_attn): Qwen3Attention(\n",
              "          (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
              "          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "          (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
              "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
              "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
              "        )\n",
              "        (mlp): Qwen3MLP(\n",
              "          (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
              "          (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
              "          (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
              "          (act_fn): SiLUActivation()\n",
              "        )\n",
              "        (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
              "        (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
              "      )\n",
              "    )\n",
              "    (norm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
              "    (rotary_emb): Qwen3RotaryEmbedding()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=1024, out_features=151936, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "validation_index = 0\n",
        "validation_template = \"Consider the following premise: {premise}.\\nDoes the premise entail the following hypothesis: {hypothesis}?\\nPlease answer with: 'yes', 'no', or 'maybe'.\\n\"\n",
        "\n",
        "row = val_ds_orig[validation_index]\n",
        "premise = row['premise']\n",
        "hypothesis = row['hypothesis']\n",
        "\n",
        "validation_text = validation_template.format(premise = premise, hypothesis = hypothesis)\n",
        "print(validation_text)\n",
        "\n",
        "full_prompt = prompt + validation_text\n",
        "print(full_prompt)\n",
        "\n",
        "inputs = tokenizer(full_prompt, return_tensors=\"pt\")\n",
        "print(inputs)\n",
        "print(inputs['input_ids'].shape)\n",
        "\n",
        "\n",
        "inputs = {k: v.to(device) for k,v in inputs.items()}\n",
        "\n",
        "out = model(**inputs)\n",
        "print(out.logits.shape)\n",
        "logits = out.logits\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\"yes\n",
        "[9693]\n",
        "\n",
        "[13]\n",
        "0s\n",
        "tokenizer.encode(\"no\")\n",
        "[2152]\n",
        "\n",
        "[14]\n",
        "0s\n",
        "tokenizer.encode(\"maybe\")\n",
        "[36760]\n",
        "\n",
        "  label_to_text_map = {\n",
        "      0: \"yes\",\n",
        "      1: \"maybe\",\n",
        "      2: \"no\"\n",
        "  }\n",
        "\n",
        "\"\"\"\n",
        "last_token_logits = logits[:, -1, [9693, 36760, 2152]]\n",
        "print(last_token_logits.shape)\n",
        "\n",
        "_, labels = torch.max(last_token_logits, dim=1)\n",
        "print(labels.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJn6U9uMscMl",
        "outputId": "f117b58a-23fa-4afd-fbdc-b1c02af73bbf"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Consider the following premise: Μεταξύ του νησιού και της ηπειρωτικής χώρας είναι η Laguna Nichupte, μια τεράστια λιμνοθάλασσα με θαλασσινό νερό, οριοθετημένη από βάλτους με μανγκρόβια, που αποτελούν καταφύγιο για πολλά είδη άγριας ζωής..\n",
            "Does the premise entail the following hypothesis: Η λίμνη Nichupte είναι 40 στρέμματα με νερό.?\n",
            "Please answer with: 'yes', 'no', or 'maybe'.\n",
            "\n",
            "Consider the following premise: There would be little benefit to national saving from allowing early access to mandatory accounts with set contribution levels-which has been proposed for Social Security (see Q4..\n",
            "Does the premise entail the following hypothesis: There would be little benefit to national saving?\n",
            "Please answer with: 'yes', 'no', or 'maybe'.\n",
            "yes\n",
            "\n",
            "Consider the following premise: เรายังไม่ได้สัมภาษณ์บุคคลที่มีความรู้ความสามารถครบทุกคนหรือยังไม่ได้เห็นรายงานที่เกี่ยวข้องทั้งหมดเลย.\n",
            "Does the premise entail the following hypothesis: ยังไม่ได้รับข้อมูลจากทุกคนที่รู้?\n",
            "Please answer with: 'yes', 'no', or 'maybe'.\n",
            "yes\n",
            "\n",
            "Consider the following premise: The most important directions are simply up and up leads eventually to the cathedral and fortress commanding the hilltop, and down inevitably leads to one of three gates through the wall to the new town..\n",
            "Does the premise entail the following hypothesis: Go downwards to one of the gates, all of which will lead you into the cathedral.?\n",
            "Please answer with: 'yes', 'no', or 'maybe'.\n",
            "no\n",
            "\n",
            "Consider the following premise: Jon walked back to the town to the smithy..\n",
            "Does the premise entail the following hypothesis: Jon continued on into the mountains.?\n",
            "Please answer with: 'yes', 'no', or 'maybe'.\n",
            "no\n",
            "\n",
            "Consider the following premise: في الوقت الحالي تم فتح ممر في التصنيف للرجال ومن هذا الممر أتت السيدة بيشوب تليها المرأة التي أسلافها من الزنوج..\n",
            "Does the premise entail the following hypothesis: مشىت الانسة بيشوب خلال مجموعة من الرجال.?\n",
            "Please answer with: 'yes', 'no', or 'maybe'.\n",
            "yes\n",
            "\n",
            "Consider the following premise: Ωωωω, είναι υπέροχη, είναι ξέρετε, είναι ένας χαρακτήρας που θα καθίσει με οποιονδήποτε, θα παίξει με οποιονδήποτε.\n",
            "Does the premise entail the following hypothesis: Αυτή συνήθως παίζει πόκερ ή μπλακ τζακ αλλά μερικές φορές παίζει και σκραμπλ.?\n",
            "Please answer with: 'yes', 'no', or 'maybe'.\n",
            "maybe\n",
            "\n",
            "Consider the following premise: yeah it's just a matter of education i think.\n",
            "Does the premise entail the following hypothesis: Yeah but education doesn't matter.?\n",
            "Please answer with: 'yes', 'no', or 'maybe'.\n",
            "no\n",
            "\n",
            "Consider the following premise: तो इससे कोई फर्क नहीं पड़ता,  केवल हंगामा होता है.\n",
            "Does the premise entail the following hypothesis: कुछ लोग अराजकता फैलाना पसंद करते हैं |?\n",
            "Please answer with: 'yes', 'no', or 'maybe'.\n",
            "maybe\n",
            "\n",
            "Consider the following premise: wow who can afford that  my God i can't afford to miss a day let alone six.\n",
            "Does the premise entail the following hypothesis: If I needed to take time off from work, I could afford it.?\n",
            "Please answer with: 'yes', 'no', or 'maybe'.\n",
            "no\n",
            "\n",
            "Consider the following premise: To control land and sea routes to the south, the Mauryas still needed to conquer the eastern kingdom of Kalinga (modern Orissa)..\n",
            "Does the premise entail the following hypothesis: The Mauryas had a large army capable of conquering Kalinga.?\n",
            "Please answer with: 'yes', 'no', or 'maybe'.\n",
            "maybe\n",
            "\n",
            "Consider the following premise: Μεταξύ του νησιού και της ηπειρωτικής χώρας είναι η Laguna Nichupte, μια τεράστια λιμνοθάλασσα με θαλασσινό νερό, οριοθετημένη από βάλτους με μανγκρόβια, που αποτελούν καταφύγιο για πολλά είδη άγριας ζωής..\n",
            "Does the premise entail the following hypothesis: Η λίμνη Nichupte είναι 40 στρέμματα με νερό.?\n",
            "Please answer with: 'yes', 'no', or 'maybe'.\n",
            "\n",
            "{'input_ids': tensor([[37175,   279,  2701,  ...,   364, 36760, 23569]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]])}\n",
            "torch.Size([1, 1127])\n",
            "torch.Size([1, 1127, 151936])\n",
            "torch.Size([1, 3])\n",
            "torch.Size([1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "val_loader = DataLoader(val_ds_orig, batch_size=4)"
      ],
      "metadata": {
        "id": "8g8QAUe4Lyf_"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "t2Mb9tcO3Ckq"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer(prompt)\n",
        "print(inputs)\n",
        "orig_len = len(inputs['input_ids'])\n",
        "print(orig_len)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gggvoe0X6vXx",
        "outputId": "c5139402-34f1-4e0f-8358-95171f1bad60"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': [37175, 279, 2701, 40202, 25, 2619, 1035, 387, 2632, 8760, 311, 5313, 13997, 504, 10693, 4124, 2615, 311, 23042, 9618, 448, 738, 18527, 5866, 12, 8206, 702, 1012, 10981, 369, 9784, 8234, 320, 4060, 1207, 19, 33947, 21468, 279, 40202, 85992, 279, 2701, 30078, 25, 2619, 1035, 387, 2632, 8760, 311, 5313, 13997, 5267, 5501, 4226, 448, 25, 364, 9693, 516, 364, 2152, 516, 476, 364, 36760, 23569, 9693, 271, 37175, 279, 2701, 40202, 25, 94482, 124878, 123874, 86032, 18625, 127196, 19841, 35648, 23271, 26283, 139344, 86348, 55770, 36142, 47642, 40327, 124358, 35884, 47171, 26283, 28319, 123971, 22287, 123959, 139084, 123886, 126829, 47642, 25200, 124256, 124618, 83581, 34509, 123874, 86032, 18625, 127196, 19841, 124272, 123909, 133643, 35884, 47171, 123949, 47171, 125293, 47839, 124010, 35884, 23271, 19841, 30434, 126686, 124776, 624, 21468, 279, 40202, 85992, 279, 2701, 30078, 25, 220, 34509, 123874, 86032, 18625, 127196, 19841, 22287, 83546, 47839, 80614, 91200, 124205, 35884, 47642, 25200, 124256, 35884, 47171, 22287, 123959, 5267, 5501, 4226, 448, 25, 364, 9693, 516, 364, 2152, 516, 476, 364, 36760, 23569, 9693, 271, 37175, 279, 2701, 40202, 25, 576, 1429, 2989, 17961, 525, 4936, 705, 323, 705, 11508, 9583, 311, 279, 79150, 323, 69838, 64340, 279, 23946, 3481, 11, 323, 1495, 39505, 11508, 311, 825, 315, 2326, 34534, 1526, 279, 7002, 311, 279, 501, 6290, 33947, 21468, 279, 40202, 85992, 279, 2701, 30078, 25, 5994, 91270, 311, 825, 315, 279, 34534, 11, 678, 315, 892, 686, 2990, 498, 1119, 279, 79150, 13, 5267, 5501, 4226, 448, 25, 364, 9693, 516, 364, 2152, 516, 476, 364, 36760, 23569, 2152, 271, 37175, 279, 2701, 40202, 25, 12285, 14858, 1182, 311, 279, 6290, 311, 279, 76721, 88, 33947, 21468, 279, 40202, 85992, 279, 2701, 30078, 25, 12285, 8570, 389, 1119, 279, 23501, 13, 5267, 5501, 4226, 448, 25, 364, 9693, 516, 364, 2152, 516, 476, 364, 36760, 23569, 2152, 271, 37175, 279, 2701, 40202, 25, 77273, 129816, 134898, 128510, 45577, 123997, 23364, 124122, 77273, 94957, 41593, 124034, 20931, 125006, 11071, 124944, 128562, 128280, 53479, 124122, 63415, 14293, 14293, 123913, 130462, 27846, 126464, 124495, 39434, 123897, 124006, 133468, 128261, 128962, 125395, 124006, 63237, 125572, 11798, 124146, 33947, 21468, 279, 40202, 85992, 279, 2701, 30078, 25, 125709, 55057, 14293, 125696, 124966, 27846, 126464, 124495, 128374, 131175, 63237, 142841, 13, 5267, 5501, 4226, 448, 25, 364, 9693, 516, 364, 2152, 516, 476, 364, 36760, 23569, 9693, 271, 37175, 279, 2701, 40202, 25, 7851, 102, 56871, 56871, 56871, 11, 59147, 54141, 33269, 88902, 17383, 227, 48245, 79431, 38079, 27554, 89102, 41424, 11, 59147, 54141, 33269, 88902, 7851, 122, 79431, 38079, 30143, 35824, 30143, 11, 59147, 54141, 33269, 88902, 7851, 255, 33269, 18945, 45642, 17383, 229, 18945, 38079, 18945, 67337, 35824, 72930, 38079, 18945, 45642, 51745, 72886, 7851, 116, 18945, 71638, 18945, 88538, 54141, 43928, 30143, 29762, 32883, 30143, 7851, 123, 48245, 27554, 29762, 27554, 33269, 85386, 72930, 48245, 27554, 35824, 30143, 11, 7851, 116, 18945, 51745, 18945, 54141, 145122, 30143, 29762, 32883, 30143, 7851, 123, 48245, 27554, 29762, 27554, 33269, 85386, 72930, 48245, 27554, 35824, 30143, 624, 21468, 279, 40202, 85992, 279, 2701, 30078, 25, 7851, 239, 53456, 35824, 72930, 47723, 53456, 33269, 72930, 88538, 56871, 45642, 51745, 18945, 54141, 145169, 30143, 29762, 51745, 75195, 67337, 30143, 38079, 7851, 106, 32883, 48245, 33486, 18945, 67337, 38470, 145169, 18945, 67337, 19043, 33486, 33486, 74134, 32883, 30143, 38079, 29762, 67337, 79431, 45642, 97875, 27554, 38079, 79431, 45642, 51745, 18945, 54141, 145169, 30143, 29762, 71638, 88902, 47723, 67337, 38079, 18945, 43123, 48245, 33486, 13, 5267, 5501, 4226, 448, 25, 364, 9693, 516, 364, 2152, 516, 476, 364, 36760, 23569, 36760, 271, 37175, 279, 2701, 40202, 25, 21639, 432, 594, 1101, 264, 4925, 315, 6731, 600, 1744, 624, 21468, 279, 40202, 85992, 279, 2701, 30078, 25, 21607, 714, 6731, 3171, 944, 4925, 13, 5267, 5501, 4226, 448, 25, 364, 9693, 516, 364, 2152, 516, 476, 364, 36760, 23569, 2152, 271, 37175, 279, 2701, 40202, 25, 14925, 97, 54575, 14925, 229, 78368, 78368, 34370, 47809, 54575, 147131, 14925, 104, 44179, 30484, 243, 14925, 101, 93948, 43647, 72314, 83636, 146187, 5502, 120, 79238, 23868, 11, 220, 47809, 54784, 113, 91811, 84310, 72314, 145959, 31411, 106, 23868, 84310, 54575, 79238, 23868, 84310, 12619, 230, 624, 21468, 279, 40202, 85992, 279, 2701, 30078, 25, 47809, 72653, 147877, 14925, 110, 54575, 145959, 14925, 227, 44179, 31411, 250, 64704, 79238, 23868, 14925, 104, 12619, 230, 91811, 31411, 101, 23868, 83636, 78368, 72314, 145256, 47809, 44179, 79238, 34370, 84310, 12619, 230, 72314, 760, 5267, 5501, 4226, 448, 25, 364, 9693, 516, 364, 2152, 516, 476, 364, 36760, 23569, 36760, 271, 37175, 279, 2701, 40202, 25, 35665, 879, 646, 9946, 429, 220, 847, 4264, 600, 646, 944, 9946, 311, 3116, 264, 1899, 1077, 7484, 4743, 624, 21468, 279, 40202, 85992, 279, 2701, 30078, 25, 1416, 358, 4362, 311, 1896, 882, 1007, 504, 975, 11, 358, 1410, 9946, 432, 13, 5267, 5501, 4226, 448, 25, 364, 9693, 516, 364, 2152, 516, 476, 364, 36760, 23569, 2152, 271, 37175, 279, 2701, 40202, 25, 2014, 2524, 4268, 323, 9396, 11291, 311, 279, 9806, 11, 279, 11331, 3350, 300, 2058, 4362, 311, 50530, 279, 23149, 25079, 315, 730, 6132, 64, 320, 49789, 2521, 21429, 8, 33947, 21468, 279, 40202, 85992, 279, 2701, 30078, 25, 576, 11331, 3350, 300, 1030, 264, 3460, 13390, 12875, 315, 38779, 4671, 730, 6132, 64, 13, 5267, 5501, 4226, 448, 25, 364, 9693, 516, 364, 2152, 516, 476, 364, 36760, 23569, 36760, 271], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
            "897\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "tokenizer.padding_side = 'left'\n",
        "acc = 0\n",
        "\n",
        "total_num_correct = 0\n",
        "total_samples = 0\n",
        "\n",
        "model.eval()\n",
        "\n",
        "\n",
        "for batch in tqdm(val_loader):\n",
        "  prompts = []\n",
        "\n",
        "  for i in range(len(batch['label'])):\n",
        "    premise = batch['premise'][i]\n",
        "    hypothesis = batch['hypothesis'][i]\n",
        "\n",
        "    validation_text = validation_template.format(premise = premise, hypothesis = hypothesis)\n",
        "    full_prompt = prompt + validation_text\n",
        "\n",
        "    prompts.append(full_prompt)\n",
        "\n",
        "\n",
        "  labels = torch.tensor(batch['label']).to(device)\n",
        "\n",
        "  # create inputs\n",
        "  inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True)\n",
        "  inputs = {k: v.to(device) for k,v in inputs.items()}\n",
        "\n",
        "  # generations_list = []\n",
        "  # for i in range(len(generations)):\n",
        "\n",
        "  #   orig_len = len(inputs['input_ids'][0])\n",
        "  #   tensor = generations[i].cpu().tolist()[orig_len: ]\n",
        "  #   decoded_text = tokenizer.decode(tensor)\n",
        "  #   generations_list.append((tensor, decoded_text))\n",
        "\n",
        "  # print(len(generations_list))\n",
        "  # for tensor, decoded_text in generations_list:\n",
        "  #   print(tensor)\n",
        "  #   print(decoded_text)\n",
        "  #   print(\"---------------\")\n",
        "\n",
        "  # break\n",
        "\n",
        "  with torch.no_grad():\n",
        "    out = model(**inputs)\n",
        "    logits = out.logits\n",
        "\n",
        "    last_token_logits = logits[:, -1, [9693, 36760, 2152]]\n",
        "    _, preds = torch.max(last_token_logits, dim=1)\n",
        "\n",
        "  # print(f\"len(prompts): {len(prompts)}\")\n",
        "  # print(last_token_logits.shape)\n",
        "\n",
        "  # print(preds.shape)\n",
        "  # print(labels.shape)\n",
        "\n",
        "  num_correct = torch.sum((preds == labels).to(torch.int)).item()\n",
        "\n",
        "  total_num_correct += num_correct\n",
        "  total_samples += len(preds)\n",
        "\n",
        "\n",
        "print(f\"Final accuracy: {total_num_correct}/{total_samples} - {total_num_correct / total_samples}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GERfYFeUzJbv",
        "outputId": "aeea8d1d-d734-4d9d-adff-a6b60dc51dd0"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/303 [00:00<?, ?it/s]/tmp/ipython-input-554111792.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(batch['label']).to(device)\n",
            "100%|██████████| 303/303 [01:49<00:00,  2.77it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final accuracy: 669/1212 - 0.551980198019802\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# loop through the test dataset\n",
        "\n",
        "# collect the answers\n",
        "\n",
        "# write submission file"
      ],
      "metadata": {
        "id": "RbJuxMV7sJpO"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kfT46kib8X1m"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}